{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5537e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import json\n",
    "\n",
    "# Initialize GCS client\n",
    "gcp_credentials_info = os.getenv(\"GCP_SERVICE_ACCOUNT_CREDENTIALS\")\n",
    "if gcp_credentials_info:\n",
    "    gcp_credentials_info = json.loads(gcp_credentials_info)\n",
    "    gcp_service_account_credentials = service_account.Credentials.from_service_account_info(gcp_credentials_info)\n",
    "    storage_client = storage.Client(credentials=gcp_service_account_credentials)\n",
    "else:\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "# Fetch all JSON files from the articoli_nt+fisco folder and extract URLs\n",
    "bucket = storage_client.bucket(\"loomy-jobs\")\n",
    "# bucket = storage_client.bucket(\"loomy-public-documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f00bd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting files starting with capital letter: 100%|██████████| 3517/3517 [17:31<00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Deleted 3512 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete all files whose filename starts with a capital letter\n",
    "blobs = list(bucket.list_blobs(prefix=\"nt_fisco/\"))\n",
    "deleted_count = 0\n",
    "\n",
    "for blob in tqdm(blobs, desc=\"Deleting files starting with capital letter\"):\n",
    "    filename = blob.name.split(\"/\")[-1]\n",
    "    if filename and filename[0].isupper():\n",
    "        blob.delete()\n",
    "        deleted_count += 1\n",
    "\n",
    "print(f\"Done. Deleted {deleted_count} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f9ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Renaming files: 100%|██████████| 1768/1768 [08:46<00:00,  3.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Renamed 1767 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename files by removing the trailing ID (e.g., \"_5b07a2f9d2dcae34.json\" -> \".json\")\n",
    "pattern = re.compile(r'_[a-f0-9]{16}\\.json$')\n",
    "\n",
    "blobs = list(bucket.list_blobs(prefix=\"articoli_nt+fisco/\"))\n",
    "renamed_count = 0\n",
    "\n",
    "for blob in tqdm(blobs, desc=\"Renaming files\"):\n",
    "    if blob.name.endswith(\".json\"):\n",
    "        filename = blob.name.split(\"/\")[-1]\n",
    "        folder = blob.name.split(\"/\")[1]\n",
    "        if pattern.search(filename):\n",
    "            new_filename = pattern.sub('.json', filename)\n",
    "            new_blob_name = f\"articoli_nt+fisco/{folder}/{new_filename}\"\n",
    "            \n",
    "            # Copy to new name and delete old\n",
    "            bucket.copy_blob(blob, bucket, new_blob_name)\n",
    "            blob.delete()\n",
    "            renamed_count += 1\n",
    "\n",
    "print(f\"Done. Renamed {renamed_count} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d406eb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1760 URLs from GCS bucket\n"
     ]
    }
   ],
   "source": [
    "blobs = bucket.list_blobs(prefix=\"articoli_nt+fisco/\")\n",
    "\n",
    "# Dict with filename as key and url as value\n",
    "url_by_filename = {}\n",
    "for blob in blobs:\n",
    "    if blob.name.endswith(\".json\"):\n",
    "        # Extract filename from the blob path (e.g., \"articoli_nt+fisco/somefile.json\" -> \"somefile.json\")\n",
    "        filename = blob.name.split(\"/\")[-1]\n",
    "        folder = blob.name.split(\"/\")[1]\n",
    "        content = blob.download_as_text()\n",
    "        data = json.loads(content)\n",
    "        if \"url\" in data:\n",
    "            url_by_filename[filename] = data[\"url\"]\n",
    "\n",
    "print(f\"Found {len(url_by_filename)} URLs from GCS bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd39bde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating txt files: 100%|██████████| 1762/1762 [05:26<00:00,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Created 1761 txt files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create .txt files from JSON files in nt_fisco folder\n",
    "blobs = list(bucket.list_blobs(prefix=\"nt_fisco/\"))\n",
    "created_count = 0\n",
    "\n",
    "for blob in tqdm(blobs, desc=\"Creating txt files\"):\n",
    "    if blob.name.endswith(\".json\"):\n",
    "        content = blob.download_as_text()\n",
    "        data = json.loads(content)\n",
    "        \n",
    "        title = data.get(\"title\", \"\")\n",
    "        preview = data.get(\"preview\", \"\")\n",
    "        \n",
    "        # Create txt content\n",
    "        txt_content = f\"\"\"{title}\n",
    "\n",
    "{preview}\n",
    "\"\"\"\n",
    "        \n",
    "        # Get filename and create new blob in txt folder\n",
    "        filename = blob.name.split(\"/\")[-1].replace(\".json\", \".txt\")\n",
    "        txt_blob_name = f\"nt_fisco/txt/{filename}\"\n",
    "        txt_blob = bucket.blob(txt_blob_name)\n",
    "        txt_blob.upload_from_string(txt_content, content_type=\"text/plain\")\n",
    "        created_count += 1\n",
    "\n",
    "print(f\"Done. Created {created_count} txt files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef25b89",
   "metadata": {},
   "source": [
    "Job cost estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50428bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating tokens: 100%|██████████| 1759/1759 [02:34<00:00, 11.41it/s]\n",
      "Estimating tokens: 100%|██████████| 1759/1759 [02:34<00:00, 11.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39084"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all the txt files in the nt_fisco/txt folder and estimate the tokens\n",
    "blobs = list(bucket.list_blobs(prefix=\"nt_fisco/txt/\"))\n",
    "total_tokens = 0\n",
    "for blob in tqdm(blobs, desc=\"Estimating tokens\"):\n",
    "    if blob.name.endswith(\".txt\"):\n",
    "        content = blob.download_as_text()\n",
    "        # Simple token estimation: 1 token = 0.75 words\n",
    "        estimated_tokens = int(len(content.split()) * 0.75)\n",
    "        total_tokens += estimated_tokens\n",
    "total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45413b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00898932"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nova micro eu-central-1\n",
    "input_price = 0.000046\n",
    "output_price = 0.000184\n",
    "total_cost = (total_tokens / 1000) * input_price + (total_tokens / 1000) * output_price\n",
    "total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39a7da",
   "metadata": {},
   "source": [
    "I need to embed before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_token = None\n",
    "updated_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "while True:\n",
    "    # fetch a page\n",
    "    res = index.query(\n",
    "        vector=[0] * 1536,                 # dummy vector\n",
    "        filter={\"source\": \"nt_fisco\"},\n",
    "        top_k=500,                         # page size\n",
    "        include_metadata=True,\n",
    "        include_values=False,\n",
    "        next_page_token=page_token\n",
    "    )\n",
    "\n",
    "    matches = res.get(\"matches\", [])\n",
    "    if not matches:\n",
    "        break\n",
    "\n",
    "    updates = []\n",
    "    for match in matches:\n",
    "        doc_name = match[\"metadata\"].get(\"doc_name\")\n",
    "        if doc_name and doc_name in url_by_filename:\n",
    "            updates.append({\n",
    "                \"id\": match[\"id\"],\n",
    "                \"metadata\": {\n",
    "                    **match[\"metadata\"],\n",
    "                    \"storage_path\": url_by_filename[doc_name]\n",
    "                }\n",
    "            })\n",
    "            updated_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "\n",
    "    # push updates to Pinecone\n",
    "    if updates:\n",
    "        index.upsert(vectors=updates)\n",
    "\n",
    "    # move to next page\n",
    "    page_token = res.get(\"next_page_token\")\n",
    "    if not page_token:\n",
    "        break\n",
    "\n",
    "print(f\"Done. Updated: {updated_count} vectors. Skipped: {skipped_count} (no matching URL).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
